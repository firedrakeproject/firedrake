{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "403628da",
   "metadata": {},
   "source": [
    "# HPC Demonstration\n",
    "\n",
    "This notebook explores some of the considerations that need to be made when running Firedrake on High Performance Computers (HPC). We will solve very large instances of the Poisson equation, demonstrating a range of different solver options for different sized problems. Additional supplimentary material is provided for running scripts on HPC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370155eb",
   "metadata": {},
   "source": [
    "## How big?\n",
    "\n",
    "The parameters `Nx`, `Nref` and `degree` defined below have been selected so that the simulation runs on a single core in a notebook. This is not the regime we want to think about in this tutorial, we want to think about very large problems. We will consider how each of these parameters affects the overall prolem size.\n",
    "\n",
    "`Nx` defines our coarse grid in the mesh hierarchy, it is used to construct a coarse cube mesh. The cube will be divided up into $N_x \\times N_x \\times N_x$ smaller cubes and each cube is then (by default) split into 6 tetrahedra giving a total of $6 N_x^3$ cells.\n",
    "\n",
    "The `Nref` parameter determines how many times that mesh is refined to create a mesh hierarchy. Each refinement uniformly divides each tetrahedron into 8 smaller tetrahedra. After $N_{ref}$ refinements there are $8^{N_{ref}}$ times as many cells.\n",
    "\n",
    "Finally, the degree, which we denote $k$ specifies the polynomial order of the basis functions used to approximate functions in our finite element space. For standard Lagrange elements in 3D each cell has $\\frac{1}{6}(k+1)(k+2)(k+3)$ degrees of freedom per cell. Some of these degrees of freedom (DOFs) are shared with neighbouring cells for a conforming finite element scheme.\n",
    "\n",
    "This small notebook example solves a problem with a large number of DOFs, but on HPC we want to solve problems orders of magnitude larger still, by the end of this notebook we will be considering problems larger than 30 000 000 DOFs. When solving problems using Firedrake in parallel, it's worth remembering that performance can be improved by adding more processes (MPI ranks) as long as the number of DOFs remains above [50 000 DOFs per core](https://firedrakeproject.org/parallelism.html#expected-performance-improvements).\n",
    "\n",
    "Later we will discuss the complexity in terms of the variable $n$, this corresponds to the total number of DOFs.\n",
    "\n",
    "---\n",
    "### Exercise:\n",
    "Can you work out the total number of DOFs for a cube mesh: coarse grid size $N_x = 8$, $N_{ref} = 2$ refinements and degree $k=2$ Lagrange finite elements?\n",
    "\n",
    "Can you find an expression to calculate the total number of DOFs for a mesh described above in terms of the coarse mesh size $N_x$, the number of refinements $N_{ref}$, the degree of the Lagrange finite element $k$ and the geometric dimension of the mesh $d$?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c9dcc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from firedrake import *\n",
    "from firedrake.petsc import PETSc\n",
    "from time import time\n",
    "\n",
    "parprint = PETSc.Sys.Print"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d97590",
   "metadata": {},
   "source": [
    "We start as always by importing Firedrake. We also define parprint to perform parallel printing as in this [demonstration](https://firedrakeproject.org/demos/parprint.py.html). Finally, we import the Python time module to benchmark the different solvers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111ba871",
   "metadata": {},
   "source": [
    "## The equations\n",
    "We will consider the Poisson equation in a 3D domain $\\Omega = [0, 1]^3$:\n",
    "\n",
    "$$\n",
    "\\left\\{\n",
    "\\begin{aligned}\n",
    "\t-\\nabla^2 u &= f && \\text{on } \\Omega,\\\\\n",
    "\tu &= 0 && \\text{on } \\partial\\Omega,\n",
    "\\end{aligned}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "Where $f$ is given by:\n",
    "\n",
    "$$\n",
    "f(x,y,z) = -\\frac{\\pi^2}{2}\n",
    "\\times\\left( 2\\cos(\\pi x) - \\cos\\left( \\frac{\\pi x}{2} \\right)\n",
    "- 2(a^2 + b^2)\\sin(\\pi x)\\tan \\left( \\frac{\\pi x}{4} \\right)  \\right)\n",
    "\\times\\sin(a\\pi y) \\sin(b\\pi z)\n",
    "$$\n",
    "\n",
    "We use this particular right hand side since it has corresponding analytic solution:\n",
    "\n",
    "$$\n",
    "u(x,y,z) =\n",
    "\\sin(\\pi x)\\tan\\left(\\frac{\\pi x}{4}\\right)\n",
    "\\sin(a\\pi y)\\sin(b\\pi z)\n",
    "$$\n",
    "Having an analytic solution allows us to compute the error in our computed solution as $\\|u_h - u\\|_{L^2}$. For this notebook we fix $a=1$ and $b=2$, feel free to try other values.\n",
    "\n",
    "The Poisson equation has the weak form: Find $u_h \\in V$ such that\n",
    "\n",
    "$$\n",
    "\\int_\\Omega \\nabla u_h\\cdot \\nabla v\\ dx = \\int_\\Omega f v\\ dx \\qquad \\forall v \\in V\n",
    "$$\n",
    "\n",
    "For the discrete function space $V$ we initially consider piecewise quadratic Lagrange elements, that is `V = FunctionSpace(mesh, \"CG\", degree)`, with `degree=2`.\n",
    "\n",
    "It is straightworward to solve the equation using Firedrake by expressing this weak form in UFL.\n",
    "We create a Python function `make_problem` which generates a `problem` object of the desired size, a function `u_h` to store the solution and the analytic solution `truth` so we can compute the $L_2$ error norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "540607f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_problem(Nx, Nref, degree):\n",
    "    # Create mesh and mesh hierarchy\n",
    "    mesh = UnitCubeMesh(Nx, Nx, Nx)\n",
    "    hierarchy = MeshHierarchy(mesh, Nref)\n",
    "    mesh = hierarchy[-1]\n",
    "    \n",
    "    # Define the function space and print the DOFs\n",
    "    V = FunctionSpace(mesh, \"CG\", degree)\n",
    "    dofs = V.dim()\n",
    "    parprint('DOFs', dofs)\n",
    "\n",
    "    u = TrialFunction(V)\n",
    "    v = TestFunction(V)\n",
    "\n",
    "    bcs = DirichletBC(V, zero(), (1, 2, 3, 4, 5, 6))\n",
    "    \n",
    "    # Define the RHS and analytic solution\n",
    "    x, y, z = SpatialCoordinate(mesh)\n",
    "\n",
    "    a = Constant(1)\n",
    "    b = Constant(2)\n",
    "    exact = sin(pi*x)*tan(pi*x/4)*sin(a*pi*y)*sin(b*pi*z)\n",
    "    truth = Function(V).interpolate(exact)\n",
    "    f = -pi**2 / 2\n",
    "    f *= 2*cos(pi*x) - cos(pi*x/2) - 2*(a**2 + b**2)*sin(pi*x)*tan(pi*x/4)\n",
    "    f *= sin(a*pi*y)*sin(b*pi*z)\n",
    "    \n",
    "    # Define the problem using the bilinear form `a` and linear functional `L`\n",
    "    a = dot(grad(u), grad(v))*dx\n",
    "    L = f*v*dx\n",
    "    u_h = Function(V)\n",
    "    problem = LinearVariationalProblem(a, L, u_h, bcs=bcs)\n",
    "    return problem, u_h, truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2918a991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOFs 274625\n"
     ]
    }
   ],
   "source": [
    "Nx = 8\n",
    "Nref = 2\n",
    "degree = 2\n",
    "\n",
    "problem, u_h, truth = make_problem(Nx, Nref, degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f66f00f",
   "metadata": {},
   "source": [
    "Creating a problem instance we can see there are just short of 275000 DOFs. (How close was your answer to the above exercise?)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3056e647",
   "metadata": {},
   "source": [
    "## The Solver\n",
    "\n",
    "We define another function to wrap the solve, so we can provide different solver options and to assess their performance, the run time is printed.\n",
    "This is a fairly crude way to profile our code, for a more in depth guide to profiling, take a look at the page on [optimising Firedrake performance](https://firedrakeproject.org/optimising.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1052160",
   "metadata": {},
   "source": [
    "This table summarised the different solvers we will use:\n",
    "\n",
    "| Solver | Abbreviation |   Cost   | Information |\n",
    "|:-------|:------------:|:--------:|:------------|\n",
    "| LU     | LU           |   O(nÂ²)  | Firedrake Default |\n",
    "| Conjugate Gradient + Geometric Multigrid V-cycle | CG + MGV | O(qn) | Sensible choice of KSP + PC |\n",
    "| Full Geometric Multigrid | FMG | O(n) | Linear complexity |\n",
    "| Matrix free FMG | Matfree FMG | O(n) | Reduced memory O(kn) |\n",
    "| Matrix free FMG with Telescoping | Telescoped matfree FMG | O(n) | Reduced memory and reduced communication |",
    "\n",
    "\n",
    "where n is the problem size and q is the number of iterations taken by an iterative method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cfcd7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_solve(problem, parameters):\n",
    "    # Create a solver ad time how long the solve takes\n",
    "    t = time()\n",
    "    solver = LinearVariationalSolver(problem, solver_parameters=parameters)\n",
    "    solver.solve()\n",
    "    parprint(\"Runtime :\", time() - t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2870011",
   "metadata": {},
   "source": [
    "## LU\n",
    "\n",
    "We can start by looking at the Firedrake's default solver options. If you don't specify any solver options a direct solver such as MUMPS will be used to perform an LU factorisation.\n",
    "\n",
    "Here we explicitly list the PETSc solver options so it's clear how the solver is set up. We also enable the `snes_view` so that PETSc prints the solver options it's using at runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "171d0ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNES Object: (firedrake_0_) 1 MPI processes\n",
      "  type: ksponly\n",
      "  maximum iterations=50, maximum function evaluations=10000\n",
      "  tolerances: relative=1e-08, absolute=1e-50, solution=1e-08\n",
      "  total number of linear solver iterations=1\n",
      "  total number of function evaluations=1\n",
      "  norm schedule ALWAYS\n",
      "  KSP Object: (firedrake_0_) 1 MPI processes\n",
      "    type: preonly\n",
      "    maximum iterations=10000, initial guess is zero\n",
      "    tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.\n",
      "    left preconditioning\n",
      "    using NONE norm type for convergence test\n",
      "  PC Object: (firedrake_0_) 1 MPI processes\n",
      "    type: lu\n",
      "      out-of-place factorization\n",
      "      tolerance for zero pivot 2.22045e-14\n",
      "      matrix ordering: external\n",
      "      factor fill ratio given 0., needed 0.        Factored matrix follows:\n",
      "          Mat Object: 1 MPI processes\n",
      "            type: mumps\n",
      "            rows=274625, cols=274625\n",
      "            package used to perform factorization: mumps\n",
      "            total: nonzeros=410915141, allocated nonzeros=410915141\n",
      "              MUMPS run parameters:\n",
      "                SYM (matrix type):                   0\n",
      "                PAR (host participation):            1\n",
      "                ICNTL(1) (output for error):         6\n",
      "                ICNTL(2) (output of diagnostic msg): 0\n",
      "                ICNTL(3) (output for global info):   0\n",
      "                ICNTL(4) (level of printing):        0\n",
      "                ICNTL(5) (input mat struct):         0\n",
      "                ICNTL(6) (matrix prescaling):        7\n",
      "                ICNTL(7) (sequential matrix ordering):7\n",
      "                ICNTL(8) (scaling strategy):        77\n",
      "                ICNTL(10) (max num of refinements):  0\n",
      "                ICNTL(11) (error analysis):          0\n",
      "                ICNTL(12) (efficiency control):                         1\n",
      "                ICNTL(13) (sequential factorization of the root node):  0\n",
      "                ICNTL(14) (percentage of estimated workspace increase): 200\n",
      "                ICNTL(18) (input mat struct):                           0\n",
      "                ICNTL(19) (Schur complement info):                      0\n",
      "                ICNTL(20) (RHS sparse pattern):                         0\n",
      "                ICNTL(21) (solution struct):                            0\n",
      "                ICNTL(22) (in-core/out-of-core facility):               0\n",
      "                ICNTL(23) (max size of memory can be allocated locally):0\n",
      "                ICNTL(24) (detection of null pivot rows):               0\n",
      "                ICNTL(25) (computation of a null space basis):          0\n",
      "                ICNTL(26) (Schur options for RHS or solution):          0\n",
      "                ICNTL(27) (blocking size for multiple RHS):             -32\n",
      "                ICNTL(28) (use parallel or sequential ordering):        1\n",
      "                ICNTL(29) (parallel ordering):                          0\n",
      "                ICNTL(30) (user-specified set of entries in inv(A)):    0\n",
      "                ICNTL(31) (factors is discarded in the solve phase):    0\n",
      "                ICNTL(33) (compute determinant):                        0\n",
      "                ICNTL(35) (activate BLR based factorization):           0\n",
      "                ICNTL(36) (choice of BLR factorization variant):        0\n",
      "                ICNTL(38) (estimated compression rate of LU factors):   333\n",
      "                CNTL(1) (relative pivoting threshold):      0.01 \n",
      "                CNTL(2) (stopping criterion of refinement): 1.49012e-08 \n",
      "                CNTL(3) (absolute pivoting threshold):      0. \n",
      "                CNTL(4) (value of static pivoting):         -1. \n",
      "                CNTL(5) (fixation for null pivots):         0. \n",
      "                CNTL(7) (dropping parameter for BLR):       0. \n",
      "                RINFO(1) (local estimated flops for the elimination after analysis): \n",
      "                  [0] 1.17762e+12 \n",
      "                RINFO(2) (local estimated flops for the assembly after factorization): \n",
      "                  [0]  8.65578e+08 \n",
      "                RINFO(3) (local estimated flops for the elimination after factorization): \n",
      "                  [0]  1.17762e+12 \n",
      "                INFO(15) (estimated size of (in MB) MUMPS internal data for running numerical factorization): \n",
      "                [0] 10925\n",
      "                INFO(16) (size of (in MB) MUMPS internal data used during numerical factorization): \n",
      "                  [0] 10925\n",
      "                INFO(23) (num of pivots eliminated on this processor after factorization): \n",
      "                  [0] 274625\n",
      "                RINFOG(1) (global estimated flops for the elimination after analysis): 1.17762e+12 \n",
      "                RINFOG(2) (global estimated flops for the assembly after factorization): 8.65578e+08 \n",
      "                RINFOG(3) (global estimated flops for the elimination after factorization): 1.17762e+12 \n",
      "                (RINFOG(12) RINFOG(13))*2^INFOG(34) (determinant): (0.,0.)*(2^0)                INFOG(3) (estimated real workspace for factors on all processors after analysis): 410915141\n",
      "                INFOG(4) (estimated integer workspace for factors on all processors after analysis): 3667278\n",
      "                INFOG(5) (estimated maximum front size in the complete tree): 6575\n",
      "                INFOG(6) (number of nodes in the complete tree): 12112\n",
      "                INFOG(7) (ordering option effectively used after analysis): 5\n",
      "                INFOG(8) (structural symmetry in percent of the permuted matrix after analysis): 100\n",
      "                INFOG(9) (total real/complex workspace to store the matrix factors after factorization): 410915141\n",
      "                INFOG(10) (total integer space store the matrix factors after factorization): 3667278\n",
      "                INFOG(11) (order of largest frontal matrix after factorization): 6575\n",
      "                INFOG(12) (number of off-diagonal pivots): 0\n",
      "                INFOG(13) (number of delayed pivots after factorization): 0\n",
      "                INFOG(14) (number of memory compress after factorization): 0\n",
      "                INFOG(15) (number of steps of iterative refinement after solution): 0\n",
      "                INFOG(16) (estimated size (in MB) of all MUMPS internal data for factorization after analysis: value on the most memory consuming processor): 10925\n",
      "                INFOG(17) (estimated size of all MUMPS internal data for factorization after analysis: sum over all processors): 10925\n",
      "                INFOG(18) (size of all MUMPS internal data allocated during factorization: value on the most memory consuming processor): 10925\n",
      "                INFOG(19) (size of all MUMPS internal data allocated during factorization: sum over all processors): 10925\n",
      "                INFOG(20) (estimated number of entries in the factors): 410915141\n",
      "                INFOG(21) (size in MB of memory effectively used during factorization - value on the most memory consuming processor): 3759\n",
      "                INFOG(22) (size in MB of memory effectively used during factorization - sum over all processors): 3759\n",
      "                INFOG(23) (after analysis: value of ICNTL(6) effectively used): 0\n",
      "                INFOG(24) (after analysis: value of ICNTL(12) effectively used): 1\n",
      "                INFOG(25) (after factorization: number of pivots modified by static pivoting): 0\n",
      "                INFOG(28) (after factorization: number of null pivots encountered): 0\n",
      "                INFOG(29) (after factorization: effective number of entries in the factors (sum over all processors)): 410915141\n",
      "                INFOG(30, 31) (after solution: size in Mbytes of memory used during solution phase): 10907, 10907\n",
      "                INFOG(32) (after analysis: type of analysis done): 1\n",
      "                INFOG(33) (value used for ICNTL(8)): 7\n",
      "                INFOG(34) (exponent of the determinant if determinant is requested): 0\n",
      "                INFOG(35) (after factorization: number of entries taking into account BLR factor compression - sum over all processors): 410915141\n",
      "                INFOG(36) (after analysis: estimated size of all MUMPS internal data for running BLR in-core - value on the most memory consuming processor): 0\n",
      "                INFOG(37) (after analysis: estimated size of all MUMPS internal data for running BLR in-core - sum over all processors): 0\n",
      "                INFOG(38) (after analysis: estimated size of all MUMPS internal data for running BLR out-of-core - value on the most memory consuming processor): 0\n",
      "                INFOG(39) (after analysis: estimated size of all MUMPS internal data for running BLR out-of-core - sum over all processors): 0\n",
      "    linear system matrix = precond matrix:\n",
      "    Mat Object: (firedrake_0_) 1 MPI processes\n",
      "      type: seqaij\n",
      "      rows=274625, cols=274625\n",
      "      total: nonzeros=7678721, allocated nonzeros=7678721\n",
      "      total number of mallocs used during MatSetValues calls=0\n",
      "        not using I-node routines\n",
      "Runtime : 145.04979491233826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error   : 1.7378060441226638e-05\n"
     ]
    }
   ],
   "source": [
    "u_h.assign(0)\n",
    "lu_mumps = {\n",
    "    \"snes_view\": None,\n",
    "    \"ksp_type\": \"preonly\",\n",
    "    \"pc_type\": \"lu\",\n",
    "    \"pc_factor_mat_solver_type\": \"mumps\"\n",
    "}\n",
    "run_solve(problem, lu_mumps)\n",
    "parprint(\"Error   :\", errornorm(truth, u_h))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f08260",
   "metadata": {},
   "source": [
    "The above solve takes under a minute on a single Zen2 core of ARCHER2.\n",
    "\n",
    "Dense LU factorisations are very expensive, typically $O(n^3)$. Sparse LU with a state of the art solver like MUMPS or SuperLU_dist can do better, typically in $O(n^2)$ or possibly faster, depending on the specific problem. \n",
    "\n",
    "We can measure the computational cost of our solvers by increasing the problem size (the number of DOFs) and observing how this changes the solver run time. In the computational cost plots below you can see that the cost of LU factorisation stays below $O(n^{5/3})$, but this cost grows far faster than the other solver methods.\n",
    "\n",
    "Direct solvers are very fast for small problems, which is why LU is the default solver in Firedrake. However, when $n$ gets large, direct solvers are no longer viable and should be avoided where possible.\n",
    "\n",
    "![](image/hpc_single.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b97914",
   "metadata": {},
   "source": [
    "## Iterative solvers\n",
    "\n",
    "An alternative to a direct solver is an iterative solver and PETSc gives us access to a large number of Krylov Subspace solvers (KSP). Since we have a symmetric problem, we can use the Conjugate Gradient (CG) method, which has computational cost $O(qn)$, where $q$ is the number of iterations for the method to converge. \n",
    "\n",
    "To reduce $q$ we can precondition the KSP. Looking at the `make_problem` function, we have created a `MeshHierarchy` which allows for the use of a Geometric Multigrid V-cycles to precondition the CG method. The solver options for this setup are shown below.\n",
    "\n",
    "We assign 0 to the function `u_h` before we solve so that we aren't using the solution from the LU solve above as our initial guess for the CG solver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cd27a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNES Object: (firedrake_1_) 1 MPI processes\n",
      "  type: ksponly\n",
      "  maximum iterations=50, maximum function evaluations=10000\n",
      "  tolerances: relative=1e-08, absolute=1e-50, solution=1e-08\n",
      "  total number of linear solver iterations=5\n",
      "  total number of function evaluations=1\n",
      "  norm schedule ALWAYS\n",
      "  KSP Object: (firedrake_1_) 1 MPI processes\n",
      "    type: cg\n",
      "    maximum iterations=10000, initial guess is zero\n",
      "    tolerances:  relative=1e-07, absolute=1e-50, divergence=10000.\n",
      "    left preconditioning\n",
      "    using PRECONDITIONED norm type for convergence test\n",
      "  PC Object: (firedrake_1_) 1 MPI processes\n",
      "    type: mg\n",
      "      type is MULTIPLICATIVE, levels=3 cycles=v\n",
      "        Cycles per PCApply=1\n",
      "        Not using Galerkin computed coarse grid matrices\n",
      "    Coarse grid solver -- level -------------------------------\n",
      "      KSP Object: (firedrake_1_mg_coarse_) 1 MPI processes\n",
      "        type: preonly\n",
      "        maximum iterations=10000, initial guess is zero\n",
      "        tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.\n",
      "        left preconditioning\n",
      "        using NONE norm type for convergence test\n",
      "      PC Object: (firedrake_1_mg_coarse_) 1 MPI processes\n",
      "        type: lu\n",
      "          out-of-place factorization\n",
      "          tolerance for zero pivot 2.22045e-14\n",
      "          using diagonal shift on blocks to prevent zero pivot [INBLOCKS]\n",
      "          matrix ordering: nd\n",
      "          factor fill ratio given 5., needed 11.0559\n",
      "            Factored matrix follows:\n",
      "              Mat Object: 1 MPI processes\n",
      "                type: seqaij\n",
      "                rows=4913, cols=4913\n",
      "                package used to perform factorization: petsc\n",
      "                total: nonzeros=1401727, allocated nonzeros=1401727\n",
      "                  not using I-node routines\n",
      "        linear system matrix = precond matrix:\n",
      "        Mat Object: 1 MPI processes\n",
      "          type: seqaij\n",
      "          rows=4913, cols=4913\n",
      "          total: nonzeros=126785, allocated nonzeros=126785\n",
      "          total number of mallocs used during MatSetValues calls=0\n",
      "            not using I-node routines\n",
      "    Down solver (pre-smoother) on level 1 -------------------------------\n",
      "      KSP Object: (firedrake_1_mg_levels_1_) 1 MPI processes\n",
      "        type: chebyshev\n",
      "          eigenvalue estimates used:  min = 0.0995781, max = 1.09536\n",
      "          eigenvalues estimate via gmres min 0.0419329, max 0.995781\n",
      "          eigenvalues estimated using gmres with translations  [0. 0.1; 0. 1.1]          KSP Object: (firedrake_1_mg_levels_1_esteig_) 1 MPI processes\n",
      "            type: gmres\n",
      "              restart=30, using Classical (unmodified) Gram-Schmidt Orthogonalization with no iterative refinement\n",
      "              happy breakdown tolerance 1e-30\n",
      "            maximum iterations=10, initial guess is zero\n",
      "            tolerances:  relative=1e-12, absolute=1e-50, divergence=10000.\n",
      "            left preconditioning\n",
      "            using PRECONDITIONED norm type for convergence test\n",
      "          estimating eigenvalues using noisy right hand side\n",
      "        maximum iterations=2, nonzero initial guess\n",
      "        tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.\n",
      "        left preconditioning\n",
      "        using NONE norm type for convergence test\n",
      "      PC Object: (firedrake_1_mg_levels_1_) 1 MPI processes\n",
      "        type: sor\n",
      "          type = local_symmetric, iterations = 1, local iterations = 1, omega = 1.\n",
      "        linear system matrix = precond matrix:\n",
      "        Mat Object: 1 MPI processes\n",
      "          type: seqaij\n",
      "          rows=35937, cols=35937\n",
      "          total: nonzeros=977793, allocated nonzeros=977793\n",
      "          total number of mallocs used during MatSetValues calls=0\n",
      "            not using I-node routines\n",
      "    Up solver (post-smoother) same as down solver (pre-smoother)\n",
      "    Down solver (pre-smoother) on level 2 -------------------------------\n",
      "      KSP Object: (firedrake_1_mg_levels_2_) 1 MPI processes\n",
      "        type: chebyshev\n",
      "          eigenvalue estimates used:  min = 0.0995432, max = 1.09497\n",
      "          eigenvalues estimate via gmres min 0.0287731, max 0.995432\n",
      "          eigenvalues estimated using gmres with translations  [0. 0.1; 0. 1.1]          KSP Object: (firedrake_1_mg_levels_2_esteig_) 1 MPI processes\n",
      "            type: gmres\n",
      "              restart=30, using Classical (unmodified) Gram-Schmidt Orthogonalization with no iterative refinement\n",
      "              happy breakdown tolerance 1e-30\n",
      "            maximum iterations=10, initial guess is zero\n",
      "            tolerances:  relative=1e-12, absolute=1e-50, divergence=10000.\n",
      "            left preconditioning\n",
      "            using PRECONDITIONED norm type for convergence test\n",
      "          estimating eigenvalues using noisy right hand side\n",
      "        maximum iterations=2, nonzero initial guess\n",
      "        tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.\n",
      "        left preconditioning\n",
      "        using NONE norm type for convergence test\n",
      "      PC Object: (firedrake_1_mg_levels_2_) 1 MPI processes\n",
      "        type: sor\n",
      "          type = local_symmetric, iterations = 1, local iterations = 1, omega = 1.\n",
      "        linear system matrix = precond matrix:\n",
      "        Mat Object: (firedrake_1_) 1 MPI processes\n",
      "          type: seqaij\n",
      "          rows=274625, cols=274625\n",
      "          total: nonzeros=7678721, allocated nonzeros=7678721\n",
      "          total number of mallocs used during MatSetValues calls=0\n",
      "            not using I-node routines\n",
      "    Up solver (post-smoother) same as down solver (pre-smoother)\n",
      "    linear system matrix = precond matrix:\n",
      "    Mat Object: (firedrake_1_) 1 MPI processes\n",
      "      type: seqaij\n",
      "      rows=274625, cols=274625\n",
      "      total: nonzeros=7678721, allocated nonzeros=7678721\n",
      "      total number of mallocs used during MatSetValues calls=0\n",
      "        not using I-node routines\n",
      "Runtime : 31.186501026153564\n",
      "Error   : 1.737899844556488e-05\n"
     ]
    }
   ],
   "source": [
    "u_h.assign(0)\n",
    "vmg = {\n",
    "    \"snes_view\": None,\n",
    "    \"ksp_type\": \"cg\",\n",
    "    \"pc_type\": \"mg\",\n",
    "    \"pc_mg_log\": None\n",
    "}\n",
    "run_solve(problem, vmg)\n",
    "parprint(\"Error   :\", errornorm(truth, u_h))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4955120",
   "metadata": {},
   "source": [
    "The CG solver is significantly faster than the LU factorisation, but is still slower than the full Geometric multigrid method.\n",
    "\n",
    "We can measure the weak scaling performance of the solvers by increasing the size of the problem in line with the number of processors. This is done approximately in the plot below, the number of DOFs per core is displayed underneath each data point. For a solver that weak scales perfectly, when we use twice as many cores to solve a problem twice as large, it should take the same total time and the lines in the plot should be approximately constant.\n",
    "\n",
    "In the weak scaling plot below, CG weaks scales for longer than the LU factorisation, but we also see that CG in _this_ setup does not weak scale as well as the full multigrid methods.\n",
    "\n",
    "![](image/hpc_weak.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5b7c68",
   "metadata": {},
   "source": [
    "## Geometric Multigrid\n",
    "\n",
    "It is possible to solve the Poisson problem using multigrid alone, without an iterative solver. To achieve this the `ksp_type` is set to `preonly` and we now perform a number of full multigrid sweeps (sometimes called F-cycles).\n",
    "\n",
    "Beware that using `preonly` we turn off PETSc's internal convergence checks, in this example we have carefully chosen the number of smoothing steps (`mg_levels_ksp_max_it`) so that our solver converges.\n",
    "\n",
    "---\n",
    "### Exercise:\n",
    "Can we get away with fewer smoothing steps in this multigrid solver?\n",
    "Try changing `mg_levels_ksp_max_it` in the solver options and observe what happens to the errornorm for the solution.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9750d74b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNES Object: (firedrake_2_) 1 MPI processes\n",
      "  type: ksponly\n",
      "  maximum iterations=50, maximum function evaluations=10000\n",
      "  tolerances: relative=1e-08, absolute=1e-50, solution=1e-08\n",
      "  total number of linear solver iterations=1\n",
      "  total number of function evaluations=1\n",
      "  norm schedule ALWAYS\n",
      "  KSP Object: (firedrake_2_) 1 MPI processes\n",
      "    type: preonly\n",
      "    maximum iterations=10000, initial guess is zero\n",
      "    tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.\n",
      "    left preconditioning\n",
      "    using NONE norm type for convergence test\n",
      "  PC Object: (firedrake_2_) 1 MPI processes\n",
      "    type: mg\n",
      "      type is FULL, levels=3 cycles=v\n",
      "        Not using Galerkin computed coarse grid matrices\n",
      "    Coarse grid solver -- level -------------------------------\n",
      "      KSP Object: (firedrake_2_mg_coarse_) 1 MPI processes\n",
      "        type: preonly\n",
      "        maximum iterations=10000, initial guess is zero\n",
      "        tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.\n",
      "        left preconditioning\n",
      "        using NONE norm type for convergence test\n",
      "      PC Object: (firedrake_2_mg_coarse_) 1 MPI processes\n",
      "        type: lu\n",
      "          out-of-place factorization\n",
      "          tolerance for zero pivot 2.22045e-14\n",
      "          using diagonal shift on blocks to prevent zero pivot [INBLOCKS]\n",
      "          matrix ordering: external\n",
      "          factor fill ratio given 0., needed 0.            Factored matrix follows:\n",
      "              Mat Object: 1 MPI processes\n",
      "                type: mumps\n",
      "                rows=4913, cols=4913\n",
      "                package used to perform factorization: mumps\n",
      "                total: nonzeros=1881395, allocated nonzeros=1881395\n",
      "                  MUMPS run parameters:\n",
      "                    SYM (matrix type):                   0\n",
      "                    PAR (host participation):            1\n",
      "                    ICNTL(1) (output for error):         6\n",
      "                    ICNTL(2) (output of diagnostic msg): 0\n",
      "                    ICNTL(3) (output for global info):   0\n",
      "                    ICNTL(4) (level of printing):        0\n",
      "                    ICNTL(5) (input mat struct):         0\n",
      "                    ICNTL(6) (matrix prescaling):        7\n",
      "                    ICNTL(7) (sequential matrix ordering):7\n",
      "                    ICNTL(8) (scaling strategy):        77\n",
      "                    ICNTL(10) (max num of refinements):  0\n",
      "                    ICNTL(11) (error analysis):          0\n",
      "                    ICNTL(12) (efficiency control):                         1\n",
      "                    ICNTL(13) (sequential factorization of the root node):  0\n",
      "                    ICNTL(14) (percentage of estimated workspace increase): 20\n",
      "                    ICNTL(18) (input mat struct):                           0\n",
      "                    ICNTL(19) (Schur complement info):                      0\n",
      "                    ICNTL(20) (RHS sparse pattern):                         0\n",
      "                    ICNTL(21) (solution struct):                            0\n",
      "                    ICNTL(22) (in-core/out-of-core facility):               0\n",
      "                    ICNTL(23) (max size of memory can be allocated locally):0\n",
      "                    ICNTL(24) (detection of null pivot rows):               0\n",
      "                    ICNTL(25) (computation of a null space basis):          0\n",
      "                    ICNTL(26) (Schur options for RHS or solution):          0\n",
      "                    ICNTL(27) (blocking size for multiple RHS):             -32\n",
      "                    ICNTL(28) (use parallel or sequential ordering):        1\n",
      "                    ICNTL(29) (parallel ordering):                          0\n",
      "                    ICNTL(30) (user-specified set of entries in inv(A)):    0\n",
      "                    ICNTL(31) (factors is discarded in the solve phase):    0\n",
      "                    ICNTL(33) (compute determinant):                        0\n",
      "                    ICNTL(35) (activate BLR based factorization):           0\n",
      "                    ICNTL(36) (choice of BLR factorization variant):        0\n",
      "                    ICNTL(38) (estimated compression rate of LU factors):   333\n",
      "                    CNTL(1) (relative pivoting threshold):      0.01 \n",
      "                    CNTL(2) (stopping criterion of refinement): 1.49012e-08 \n",
      "                    CNTL(3) (absolute pivoting threshold):      0. \n",
      "                    CNTL(4) (value of static pivoting):         -1. \n",
      "                    CNTL(5) (fixation for null pivots):         0. \n",
      "                    CNTL(7) (dropping parameter for BLR):       0. \n",
      "                    RINFO(1) (local estimated flops for the elimination after analysis): \n",
      "                      [0] 6.41935e+08 \n",
      "                    RINFO(2) (local estimated flops for the assembly after factorization): \n",
      "                      [0]  2.43105e+06 \n",
      "                    RINFO(3) (local estimated flops for the elimination after factorization): \n",
      "                      [0]  6.41935e+08 \n",
      "                    INFO(15) (estimated size of (in MB) MUMPS internal data for running numerical factorization): \n",
      "                    [0] 22\n",
      "                    INFO(16) (size of (in MB) MUMPS internal data used during numerical factorization): \n",
      "                      [0] 22\n",
      "                    INFO(23) (num of pivots eliminated on this processor after factorization): \n",
      "                      [0] 4913\n",
      "                    RINFOG(1) (global estimated flops for the elimination after analysis): 6.41935e+08 \n",
      "                    RINFOG(2) (global estimated flops for the assembly after factorization): 2.43105e+06 \n",
      "                    RINFOG(3) (global estimated flops for the elimination after factorization): 6.41935e+08 \n",
      "                    (RINFOG(12) RINFOG(13))*2^INFOG(34) (determinant): (0.,0.)*(2^0)                    INFOG(3) (estimated real workspace for factors on all processors after analysis): 1881395\n",
      "                    INFOG(4) (estimated integer workspace for factors on all processors after analysis): 56990\n",
      "                    INFOG(5) (estimated maximum front size in the complete tree): 663\n",
      "                    INFOG(6) (number of nodes in the complete tree): 378\n",
      "                    INFOG(7) (ordering option effectively used after analysis): 2\n",
      "                    INFOG(8) (structural symmetry in percent of the permuted matrix after analysis): 100\n",
      "                    INFOG(9) (total real/complex workspace to store the matrix factors after factorization): 1881395\n",
      "                    INFOG(10) (total integer space store the matrix factors after factorization): 56990\n",
      "                    INFOG(11) (order of largest frontal matrix after factorization): 663\n",
      "                    INFOG(12) (number of off-diagonal pivots): 0\n",
      "                    INFOG(13) (number of delayed pivots after factorization): 0\n",
      "                    INFOG(14) (number of memory compress after factorization): 0\n",
      "                    INFOG(15) (number of steps of iterative refinement after solution): 0\n",
      "                    INFOG(16) (estimated size (in MB) of all MUMPS internal data for factorization after analysis: value on the most memory consuming processor): 22\n",
      "                    INFOG(17) (estimated size of all MUMPS internal data for factorization after analysis: sum over all processors): 22\n",
      "                    INFOG(18) (size of all MUMPS internal data allocated during factorization: value on the most memory consuming processor): 22\n",
      "                    INFOG(19) (size of all MUMPS internal data allocated during factorization: sum over all processors): 22\n",
      "                    INFOG(20) (estimated number of entries in the factors): 1881395\n",
      "                    INFOG(21) (size in MB of memory effectively used during factorization - value on the most memory consuming processor): 19\n",
      "                    INFOG(22) (size in MB of memory effectively used during factorization - sum over all processors): 19\n",
      "                    INFOG(23) (after analysis: value of ICNTL(6) effectively used): 0\n",
      "                    INFOG(24) (after analysis: value of ICNTL(12) effectively used): 1\n",
      "                    INFOG(25) (after factorization: number of pivots modified by static pivoting): 0\n",
      "                    INFOG(28) (after factorization: number of null pivots encountered): 0\n",
      "                    INFOG(29) (after factorization: effective number of entries in the factors (sum over all processors)): 1881395\n",
      "                    INFOG(30, 31) (after solution: size in Mbytes of memory used during solution phase): 21, 21\n",
      "                    INFOG(32) (after analysis: type of analysis done): 1\n",
      "                    INFOG(33) (value used for ICNTL(8)): 7\n",
      "                    INFOG(34) (exponent of the determinant if determinant is requested): 0\n",
      "                    INFOG(35) (after factorization: number of entries taking into account BLR factor compression - sum over all processors): 1881395\n",
      "                    INFOG(36) (after analysis: estimated size of all MUMPS internal data for running BLR in-core - value on the most memory consuming processor): 0\n",
      "                    INFOG(37) (after analysis: estimated size of all MUMPS internal data for running BLR in-core - sum over all processors): 0\n",
      "                    INFOG(38) (after analysis: estimated size of all MUMPS internal data for running BLR out-of-core - value on the most memory consuming processor): 0\n",
      "                    INFOG(39) (after analysis: estimated size of all MUMPS internal data for running BLR out-of-core - sum over all processors): 0\n",
      "        linear system matrix = precond matrix:\n",
      "        Mat Object: 1 MPI processes\n",
      "          type: seqaij\n",
      "          rows=4913, cols=4913\n",
      "          total: nonzeros=126785, allocated nonzeros=126785\n",
      "          total number of mallocs used during MatSetValues calls=0\n",
      "            not using I-node routines\n",
      "    Down solver (pre-smoother) on level 1 -------------------------------\n",
      "      KSP Object: (firedrake_2_mg_levels_1_) 1 MPI processes\n",
      "        type: chebyshev\n",
      "          eigenvalue estimates used:  min = 0.19855, max = 2.18404\n",
      "          eigenvalues estimate via gmres min 0.0436521, max 1.9855\n",
      "          eigenvalues estimated using gmres with translations  [0. 0.1; 0. 1.1]          KSP Object: (firedrake_2_mg_levels_1_esteig_) 1 MPI processes\n",
      "            type: gmres\n",
      "              restart=30, using Classical (unmodified) Gram-Schmidt Orthogonalization with no iterative refinement\n",
      "              happy breakdown tolerance 1e-30\n",
      "            maximum iterations=10, initial guess is zero\n",
      "            tolerances:  relative=1e-12, absolute=1e-50, divergence=10000.\n",
      "            left preconditioning\n",
      "            using PRECONDITIONED norm type for convergence test\n",
      "          estimating eigenvalues using noisy right hand side\n",
      "        maximum iterations=10, nonzero initial guess\n",
      "        tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.\n",
      "        left preconditioning\n",
      "        using NONE norm type for convergence test\n",
      "      PC Object: (firedrake_2_mg_levels_1_) 1 MPI processes\n",
      "        type: jacobi\n",
      "          type DIAGONAL\n",
      "        linear system matrix = precond matrix:\n",
      "        Mat Object: 1 MPI processes\n",
      "          type: seqaij\n",
      "          rows=35937, cols=35937\n",
      "          total: nonzeros=977793, allocated nonzeros=977793\n",
      "          total number of mallocs used during MatSetValues calls=0\n",
      "            not using I-node routines\n",
      "    Up solver (post-smoother) same as down solver (pre-smoother)\n",
      "    Down solver (pre-smoother) on level 2 -------------------------------\n",
      "      KSP Object: (firedrake_2_mg_levels_2_) 1 MPI processes\n",
      "        type: chebyshev\n",
      "          eigenvalue estimates used:  min = 0.199079, max = 2.18987\n",
      "          eigenvalues estimate via gmres min 0.0476982, max 1.99079\n",
      "          eigenvalues estimated using gmres with translations  [0. 0.1; 0. 1.1]          KSP Object: (firedrake_2_mg_levels_2_esteig_) 1 MPI processes\n",
      "            type: gmres\n",
      "              restart=30, using Classical (unmodified) Gram-Schmidt Orthogonalization with no iterative refinement\n",
      "              happy breakdown tolerance 1e-30\n",
      "            maximum iterations=10, initial guess is zero\n",
      "            tolerances:  relative=1e-12, absolute=1e-50, divergence=10000.\n",
      "            left preconditioning\n",
      "            using PRECONDITIONED norm type for convergence test\n",
      "          estimating eigenvalues using noisy right hand side\n",
      "        maximum iterations=10, nonzero initial guess\n",
      "        tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.\n",
      "        left preconditioning\n",
      "        using NONE norm type for convergence test\n",
      "      PC Object: (firedrake_2_mg_levels_2_) 1 MPI processes\n",
      "        type: jacobi\n",
      "          type DIAGONAL\n",
      "        linear system matrix = precond matrix:\n",
      "        Mat Object: (firedrake_2_) 1 MPI processes\n",
      "          type: seqaij\n",
      "          rows=274625, cols=274625\n",
      "          total: nonzeros=7678721, allocated nonzeros=7678721\n",
      "          total number of mallocs used during MatSetValues calls=0\n",
      "            not using I-node routines\n",
      "    Up solver (post-smoother) same as down solver (pre-smoother)\n",
      "    linear system matrix = precond matrix:\n",
      "    Mat Object: (firedrake_2_) 1 MPI processes\n",
      "      type: seqaij\n",
      "      rows=274625, cols=274625\n",
      "      total: nonzeros=7678721, allocated nonzeros=7678721\n",
      "      total number of mallocs used during MatSetValues calls=0\n",
      "        not using I-node routines\n",
      "Runtime : 22.279114246368408\n",
      "Error   : 1.9322475679852718e-05\n"
     ]
    }
   ],
   "source": [
    "u_h.assign(0)\n",
    "fmg = {\n",
    "    \"snes_view\": None,\n",
    "    \"ksp_type\": \"preonly\",\n",
    "    \"pc_type\": \"mg\",\n",
    "    \"pc_mg_log\": None,\n",
    "    \"pc_mg_type\": \"full\",\n",
    "    \"mg_levels_ksp_type\": \"chebyshev\",\n",
    "    \"mg_levels_ksp_max_it\": 10,\n",
    "    \"mg_levels_pc_type\": \"jacobi\",\n",
    "    \"mg_coarse_pc_type\": \"lu\",\n",
    "    \"mg_coarse_pc_factor_mat_solver_type\": \"mumps\"\n",
    "}\n",
    "run_solve(problem, fmg)\n",
    "parprint(\"Error   :\", errornorm(truth, u_h))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e10eb44",
   "metadata": {},
   "source": [
    "Removing the KSP iterative solve gives a significant speed up over using multigrid as a preconditioner. Note this is possible since the linear system for Poisson is relatively well conditioned, and is a prototypical problem for multigrid. In general it may not be possible to just use a multigrid preconditioner without a Kylov subspace iterative solver. Designing a fast and scalale solver for a given problem is often very challenging.\n",
    "\n",
    "We can measure the strong scaling performance of the multigrid by choosing a large enough problem and seeing how long it takes to solve on different numbers of processes. In the plot below, the number of DOFs per core is displayed underneath each data point. For a solver that strong scales perfectly, when we use twice as many cores to solve the same size problem should take half as long. This ideal scaling is also plotted as the dashed line.\n",
    "\n",
    "The figure below shows what happens when we use this multigrid solver for a large problem. For this test we set `Nx = 10` and `Nref = 4` to make a problem with 33 076 161 DOFs and solve over multiple nodes.\n",
    "\n",
    "The full multigrid solver strong scales poorly beyond 2 nodes. A matrix free solver is slightly faster, but still scales poorly. The reason for this poor scaling is that the solver spends most of its time performing communication solving the problem on the coarse grid in a distributed manner. In the next section we show how to overcome this issue using a telescoping solver.\n",
    "\n",
    "![](image/hpc_strong.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d5caa2",
   "metadata": {},
   "source": [
    "## Variants\n",
    "\n",
    "In this section we show two variants of the full multigrid solver above, which have advantages for larger problems and on HPC architectures.\n",
    "\n",
    "One key advantage of using geometric multigrid over algebraic multigrid is the ability to use matrix free methods. These methods never assemble the full finite element matrix, which for large problems gives a significant reduction in memory usage. On the coarsest mesh of the multigrid hierarchy we can use the `firedrake.AssembledPC` to assemble the finite element matrix, which allows us to use a direct solver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b6043d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNES Object: (firedrake_3_) 1 MPI processes\n",
      "  type: ksponly\n",
      "  maximum iterations=50, maximum function evaluations=10000\n",
      "  tolerances: relative=1e-08, absolute=1e-50, solution=1e-08\n",
      "  total number of linear solver iterations=1\n",
      "  total number of function evaluations=1\n",
      "  norm schedule ALWAYS\n",
      "  KSP Object: (firedrake_3_) 1 MPI processes\n",
      "    type: preonly\n",
      "    maximum iterations=10000, initial guess is zero\n",
      "    tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.\n",
      "    left preconditioning\n",
      "    using NONE norm type for convergence test\n",
      "  PC Object: (firedrake_3_) 1 MPI processes\n",
      "    type: mg\n",
      "      type is FULL, levels=3 cycles=v\n",
      "        Not using Galerkin computed coarse grid matrices\n",
      "    Coarse grid solver -- level -------------------------------\n",
      "      KSP Object: (firedrake_3_mg_coarse_) 1 MPI processes\n",
      "        type: preonly\n",
      "        maximum iterations=10000, initial guess is zero\n",
      "        tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.\n",
      "        left preconditioning\n",
      "        using NONE norm type for convergence test\n",
      "      PC Object: (firedrake_3_mg_coarse_) 1 MPI processes\n",
      "        type: python\n",
      "          Python: firedrake.AssembledPC\n",
      "        Firedrake custom preconditioner AssembledPC\n",
      "        PC to apply inverse\n",
      "        PC Object: (firedrake_3_mg_coarse_assembled_) 1 MPI processes\n",
      "          type: lu\n",
      "            out-of-place factorization\n",
      "            tolerance for zero pivot 2.22045e-14\n",
      "            matrix ordering: external\n",
      "            factor fill ratio given 0., needed 0.              Factored matrix follows:\n",
      "                Mat Object: 1 MPI processes\n",
      "                  type: mumps\n",
      "                  rows=4913, cols=4913\n",
      "                  package used to perform factorization: mumps\n",
      "                  total: nonzeros=1881395, allocated nonzeros=1881395\n",
      "                    MUMPS run parameters:\n",
      "                      SYM (matrix type):                   0\n",
      "                      PAR (host participation):            1\n",
      "                      ICNTL(1) (output for error):         6\n",
      "                      ICNTL(2) (output of diagnostic msg): 0\n",
      "                      ICNTL(3) (output for global info):   0\n",
      "                      ICNTL(4) (level of printing):        0\n",
      "                      ICNTL(5) (input mat struct):         0\n",
      "                      ICNTL(6) (matrix prescaling):        7\n",
      "                      ICNTL(7) (sequential matrix ordering):7\n",
      "                      ICNTL(8) (scaling strategy):        77\n",
      "                      ICNTL(10) (max num of refinements):  0\n",
      "                      ICNTL(11) (error analysis):          0\n",
      "                      ICNTL(12) (efficiency control):                         1\n",
      "                      ICNTL(13) (sequential factorization of the root node):  0\n",
      "                      ICNTL(14) (percentage of estimated workspace increase): 20\n",
      "                      ICNTL(18) (input mat struct):                           0\n",
      "                      ICNTL(19) (Schur complement info):                      0\n",
      "                      ICNTL(20) (RHS sparse pattern):                         0\n",
      "                      ICNTL(21) (solution struct):                            0\n",
      "                      ICNTL(22) (in-core/out-of-core facility):               0\n",
      "                      ICNTL(23) (max size of memory can be allocated locally):0\n",
      "                      ICNTL(24) (detection of null pivot rows):               0\n",
      "                      ICNTL(25) (computation of a null space basis):          0\n",
      "                      ICNTL(26) (Schur options for RHS or solution):          0\n",
      "                      ICNTL(27) (blocking size for multiple RHS):             -32\n",
      "                      ICNTL(28) (use parallel or sequential ordering):        1\n",
      "                      ICNTL(29) (parallel ordering):                          0\n",
      "                      ICNTL(30) (user-specified set of entries in inv(A)):    0\n",
      "                      ICNTL(31) (factors is discarded in the solve phase):    0\n",
      "                      ICNTL(33) (compute determinant):                        0\n",
      "                      ICNTL(35) (activate BLR based factorization):           0\n",
      "                      ICNTL(36) (choice of BLR factorization variant):        0\n",
      "                      ICNTL(38) (estimated compression rate of LU factors):   333\n",
      "                      CNTL(1) (relative pivoting threshold):      0.01 \n",
      "                      CNTL(2) (stopping criterion of refinement): 1.49012e-08 \n",
      "                      CNTL(3) (absolute pivoting threshold):      0. \n",
      "                      CNTL(4) (value of static pivoting):         -1. \n",
      "                      CNTL(5) (fixation for null pivots):         0. \n",
      "                      CNTL(7) (dropping parameter for BLR):       0. \n",
      "                      RINFO(1) (local estimated flops for the elimination after analysis): \n",
      "                        [0] 6.41935e+08 \n",
      "                      RINFO(2) (local estimated flops for the assembly after factorization): \n",
      "                        [0]  2.43105e+06 \n",
      "                      RINFO(3) (local estimated flops for the elimination after factorization): \n",
      "                        [0]  6.41935e+08 \n",
      "                      INFO(15) (estimated size of (in MB) MUMPS internal data for running numerical factorization): \n",
      "                      [0] 22\n",
      "                      INFO(16) (size of (in MB) MUMPS internal data used during numerical factorization): \n",
      "                        [0] 22\n",
      "                      INFO(23) (num of pivots eliminated on this processor after factorization): \n",
      "                        [0] 4913\n",
      "                      RINFOG(1) (global estimated flops for the elimination after analysis): 6.41935e+08 \n",
      "                      RINFOG(2) (global estimated flops for the assembly after factorization): 2.43105e+06 \n",
      "                      RINFOG(3) (global estimated flops for the elimination after factorization): 6.41935e+08 \n",
      "                      (RINFOG(12) RINFOG(13))*2^INFOG(34) (determinant): (0.,0.)*(2^0)                      INFOG(3) (estimated real workspace for factors on all processors after analysis): 1881395\n",
      "                      INFOG(4) (estimated integer workspace for factors on all processors after analysis): 56990\n",
      "                      INFOG(5) (estimated maximum front size in the complete tree): 663\n",
      "                      INFOG(6) (number of nodes in the complete tree): 378\n",
      "                      INFOG(7) (ordering option effectively used after analysis): 2\n",
      "                      INFOG(8) (structural symmetry in percent of the permuted matrix after analysis): 100\n",
      "                      INFOG(9) (total real/complex workspace to store the matrix factors after factorization): 1881395\n",
      "                      INFOG(10) (total integer space store the matrix factors after factorization): 56990\n",
      "                      INFOG(11) (order of largest frontal matrix after factorization): 663\n",
      "                      INFOG(12) (number of off-diagonal pivots): 0\n",
      "                      INFOG(13) (number of delayed pivots after factorization): 0\n",
      "                      INFOG(14) (number of memory compress after factorization): 0\n",
      "                      INFOG(15) (number of steps of iterative refinement after solution): 0\n",
      "                      INFOG(16) (estimated size (in MB) of all MUMPS internal data for factorization after analysis: value on the most memory consuming processor): 22\n",
      "                      INFOG(17) (estimated size of all MUMPS internal data for factorization after analysis: sum over all processors): 22\n",
      "                      INFOG(18) (size of all MUMPS internal data allocated during factorization: value on the most memory consuming processor): 22\n",
      "                      INFOG(19) (size of all MUMPS internal data allocated during factorization: sum over all processors): 22\n",
      "                      INFOG(20) (estimated number of entries in the factors): 1881395\n",
      "                      INFOG(21) (size in MB of memory effectively used during factorization - value on the most memory consuming processor): 19\n",
      "                      INFOG(22) (size in MB of memory effectively used during factorization - sum over all processors): 19\n",
      "                      INFOG(23) (after analysis: value of ICNTL(6) effectively used): 0\n",
      "                      INFOG(24) (after analysis: value of ICNTL(12) effectively used): 1\n",
      "                      INFOG(25) (after factorization: number of pivots modified by static pivoting): 0\n",
      "                      INFOG(28) (after factorization: number of null pivots encountered): 0\n",
      "                      INFOG(29) (after factorization: effective number of entries in the factors (sum over all processors)): 1881395\n",
      "                      INFOG(30, 31) (after solution: size in Mbytes of memory used during solution phase): 21, 21\n",
      "                      INFOG(32) (after analysis: type of analysis done): 1\n",
      "                      INFOG(33) (value used for ICNTL(8)): 7\n",
      "                      INFOG(34) (exponent of the determinant if determinant is requested): 0\n",
      "                      INFOG(35) (after factorization: number of entries taking into account BLR factor compression - sum over all processors): 1881395\n",
      "                      INFOG(36) (after analysis: estimated size of all MUMPS internal data for running BLR in-core - value on the most memory consuming processor): 0\n",
      "                      INFOG(37) (after analysis: estimated size of all MUMPS internal data for running BLR in-core - sum over all processors): 0\n",
      "                      INFOG(38) (after analysis: estimated size of all MUMPS internal data for running BLR out-of-core - value on the most memory consuming processor): 0\n",
      "                      INFOG(39) (after analysis: estimated size of all MUMPS internal data for running BLR out-of-core - sum over all processors): 0\n",
      "          linear system matrix = precond matrix:\n",
      "          Mat Object: (firedrake_3_mg_coarse_assembled_) 1 MPI processes\n",
      "            type: seqaij\n",
      "            rows=4913, cols=4913\n",
      "            total: nonzeros=126785, allocated nonzeros=126785\n",
      "            total number of mallocs used during MatSetValues calls=0\n",
      "              not using I-node routines\n",
      "        linear system matrix = precond matrix:\n",
      "        Mat Object: 1 MPI processes\n",
      "          type: python\n",
      "          rows=4913, cols=4913\n",
      "              Python: firedrake.matrix_free.operators.ImplicitMatrixContext\n",
      "            Firedrake matrix-free operator ImplicitMatrixContext\n",
      "    Down solver (pre-smoother) on level 1 -------------------------------\n",
      "      KSP Object: (firedrake_3_mg_levels_1_) 1 MPI processes\n",
      "        type: chebyshev\n",
      "          eigenvalue estimates used:  min = 0.19855, max = 2.18404\n",
      "          eigenvalues estimate via gmres min 0.0436521, max 1.9855\n",
      "          eigenvalues estimated using gmres with translations  [0. 0.1; 0. 1.1]          KSP Object: (firedrake_3_mg_levels_1_esteig_) 1 MPI processes\n",
      "            type: gmres\n",
      "              restart=30, using Classical (unmodified) Gram-Schmidt Orthogonalization with no iterative refinement\n",
      "              happy breakdown tolerance 1e-30\n",
      "            maximum iterations=10, initial guess is zero\n",
      "            tolerances:  relative=1e-12, absolute=1e-50, divergence=10000.\n",
      "            left preconditioning\n",
      "            using PRECONDITIONED norm type for convergence test\n",
      "          estimating eigenvalues using noisy right hand side\n",
      "        maximum iterations=10, nonzero initial guess\n",
      "        tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.\n",
      "        left preconditioning\n",
      "        using NONE norm type for convergence test\n",
      "      PC Object: (firedrake_3_mg_levels_1_) 1 MPI processes\n",
      "        type: jacobi\n",
      "          type DIAGONAL\n",
      "        linear system matrix = precond matrix:\n",
      "        Mat Object: 1 MPI processes\n",
      "          type: python\n",
      "          rows=35937, cols=35937\n",
      "              Python: firedrake.matrix_free.operators.ImplicitMatrixContext\n",
      "            Firedrake matrix-free operator ImplicitMatrixContext\n",
      "    Up solver (post-smoother) same as down solver (pre-smoother)\n",
      "    Down solver (pre-smoother) on level 2 -------------------------------\n",
      "      KSP Object: (firedrake_3_mg_levels_2_) 1 MPI processes\n",
      "        type: chebyshev\n",
      "          eigenvalue estimates used:  min = 0.199079, max = 2.18987\n",
      "          eigenvalues estimate via gmres min 0.0476982, max 1.99079\n",
      "          eigenvalues estimated using gmres with translations  [0. 0.1; 0. 1.1]          KSP Object: (firedrake_3_mg_levels_2_esteig_) 1 MPI processes\n",
      "            type: gmres\n",
      "              restart=30, using Classical (unmodified) Gram-Schmidt Orthogonalization with no iterative refinement\n",
      "              happy breakdown tolerance 1e-30\n",
      "            maximum iterations=10, initial guess is zero\n",
      "            tolerances:  relative=1e-12, absolute=1e-50, divergence=10000.\n",
      "            left preconditioning\n",
      "            using PRECONDITIONED norm type for convergence test\n",
      "          estimating eigenvalues using noisy right hand side\n",
      "        maximum iterations=10, nonzero initial guess\n",
      "        tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.\n",
      "        left preconditioning\n",
      "        using NONE norm type for convergence test\n",
      "      PC Object: (firedrake_3_mg_levels_2_) 1 MPI processes\n",
      "        type: jacobi\n",
      "          type DIAGONAL\n",
      "        linear system matrix = precond matrix:\n",
      "        Mat Object: (firedrake_3_) 1 MPI processes\n",
      "          type: python\n",
      "          rows=274625, cols=274625\n",
      "              Python: firedrake.matrix_free.operators.ImplicitMatrixContext\n",
      "            Firedrake matrix-free operator ImplicitMatrixContext\n",
      "    Up solver (post-smoother) same as down solver (pre-smoother)\n",
      "    linear system matrix = precond matrix:\n",
      "    Mat Object: (firedrake_3_) 1 MPI processes\n",
      "      type: python\n",
      "      rows=274625, cols=274625\n",
      "          Python: firedrake.matrix_free.operators.ImplicitMatrixContext\n",
      "        Firedrake matrix-free operator ImplicitMatrixContext\n",
      "Runtime : 21.57845163345337\n",
      "Error   : 1.9322475679750423e-05\n"
     ]
    }
   ],
   "source": [
    "u_h.assign(0)\n",
    "fmg_matfree = {\n",
    "    \"snes_view\": None,\n",
    "    \"mat_type\": \"matfree\",\n",
    "    \"ksp_type\": \"preonly\",\n",
    "    \"pc_type\": \"mg\",\n",
    "    \"pc_mg_log\": None,\n",
    "    \"pc_mg_type\": \"full\",\n",
    "    \"mg_levels_ksp_type\": \"chebyshev\",\n",
    "    \"mg_levels_ksp_max_it\": 10,\n",
    "    \"mg_levels_pc_type\": \"jacobi\",\n",
    "    \"mg_coarse_pc_type\": \"python\",\n",
    "    \"mg_coarse_pc_python_type\": \"firedrake.AssembledPC\",\n",
    "    \"mg_coarse_assembled\": {\n",
    "        \"mat_type\": \"aij\",\n",
    "        \"pc_type\": \"lu\",\n",
    "        \"pc_factor_mat_solver_type\": \"mumps\"\n",
    "    }\n",
    "}\n",
    "run_solve(problem, fmg_matfree)\n",
    "parprint(\"Error   :\", errornorm(truth, u_h))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea794366",
   "metadata": {},
   "source": [
    "The final set of solver options deals with very large problems spread over multiple compute nodes. For a problem with a large multigrid hierarchy, the coarse grid problem is often so small that when it is solved over multiple nodes, the coarse solve spends all its time performing communication, which is slow.\n",
    "\n",
    "The solution is to let each node solve a local copy of the coarse grid problem, which avoids this communication. This functionality is enabled using the `telescope` preconditioner alongside the assembled preconditioner, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d11e2fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNES Object: (firedrake_4_) 1 MPI processes\n",
      "  type: ksponly\n",
      "  maximum iterations=50, maximum function evaluations=10000\n",
      "  tolerances: relative=1e-08, absolute=1e-50, solution=1e-08\n",
      "  total number of linear solver iterations=1\n",
      "  total number of function evaluations=1\n",
      "  norm schedule ALWAYS\n",
      "  KSP Object: (firedrake_4_) 1 MPI processes\n",
      "    type: preonly\n",
      "    maximum iterations=10000, initial guess is zero\n",
      "    tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.\n",
      "    left preconditioning\n",
      "    using NONE norm type for convergence test\n",
      "  PC Object: (firedrake_4_) 1 MPI processes\n",
      "    type: mg\n",
      "      type is FULL, levels=3 cycles=v\n",
      "        Not using Galerkin computed coarse grid matrices\n",
      "    Coarse grid solver -- level -------------------------------\n",
      "      KSP Object: (firedrake_4_mg_coarse_) 1 MPI processes\n",
      "        type: preonly\n",
      "        maximum iterations=10000, initial guess is zero\n",
      "        tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.\n",
      "        left preconditioning\n",
      "        using NONE norm type for convergence test\n",
      "      PC Object: (firedrake_4_mg_coarse_) 1 MPI processes\n",
      "        type: python\n",
      "          Python: firedrake.AssembledPC\n",
      "        Firedrake custom preconditioner AssembledPC\n",
      "        PC to apply inverse\n",
      "        PC Object: (firedrake_4_mg_coarse_assembled_) 1 MPI processes\n",
      "          type: telescope\n",
      "            petsc subcomm: parent comm size reduction factor = 1\n",
      "            petsc subcomm: parent_size = 1 , subcomm_size = 1\n",
      "            petsc subcomm type = contiguous\n",
      "            setup type: default\n",
      "            Parent DM object: type = shell;\n",
      "            Sub DM object: NULL\n",
      "            KSP Object: (firedrake_4_mg_coarse_assembled_telescope_) 1 MPI processes\n",
      "              type: preonly\n",
      "              maximum iterations=10000, initial guess is zero\n",
      "              tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.\n",
      "              left preconditioning\n",
      "              using NONE norm type for convergence test\n",
      "            PC Object: (firedrake_4_mg_coarse_assembled_telescope_) 1 MPI processes\n",
      "              type: lu\n",
      "                out-of-place factorization\n",
      "                tolerance for zero pivot 2.22045e-14\n",
      "                matrix ordering: external\n",
      "                factor fill ratio given 0., needed 0.\n",
      "                  Factored matrix follows:\n",
      "                    Mat Object: 1 MPI processes\n",
      "                      type: mumps\n",
      "                      rows=4913, cols=4913\n",
      "                      package used to perform factorization: mumps\n",
      "                      total: nonzeros=1881395, allocated nonzeros=1881395\n",
      "                        MUMPS run parameters:\n",
      "                          SYM (matrix type):                   0\n",
      "                          PAR (host participation):            1\n",
      "                          ICNTL(1) (output for error):         6\n",
      "                          ICNTL(2) (output of diagnostic msg): 0\n",
      "                          ICNTL(3) (output for global info):   0\n",
      "                          ICNTL(4) (level of printing):        0\n",
      "                          ICNTL(5) (input mat struct):         0\n",
      "                          ICNTL(6) (matrix prescaling):        7\n",
      "                          ICNTL(7) (sequential matrix ordering):7\n",
      "                          ICNTL(8) (scaling strategy):        77\n",
      "                          ICNTL(10) (max num of refinements):  0\n",
      "                          ICNTL(11) (error analysis):          0\n",
      "                          ICNTL(12) (efficiency control):                         1\n",
      "                          ICNTL(13) (sequential factorization of the root node):  0\n",
      "                          ICNTL(14) (percentage of estimated workspace increase): 20\n",
      "                          ICNTL(18) (input mat struct):                           0\n",
      "                          ICNTL(19) (Schur complement info):                      0\n",
      "                          ICNTL(20) (RHS sparse pattern):                         0\n",
      "                          ICNTL(21) (solution struct):                            0\n",
      "                          ICNTL(22) (in-core/out-of-core facility):               0\n",
      "                          ICNTL(23) (max size of memory can be allocated locally):0\n",
      "                          ICNTL(24) (detection of null pivot rows):               0\n",
      "                          ICNTL(25) (computation of a null space basis):          0\n",
      "                          ICNTL(26) (Schur options for RHS or solution):          0\n",
      "                          ICNTL(27) (blocking size for multiple RHS):             -32\n",
      "                          ICNTL(28) (use parallel or sequential ordering):        1\n",
      "                          ICNTL(29) (parallel ordering):                          0\n",
      "                          ICNTL(30) (user-specified set of entries in inv(A)):    0\n",
      "                          ICNTL(31) (factors is discarded in the solve phase):    0\n",
      "                          ICNTL(33) (compute determinant):                        0\n",
      "                          ICNTL(35) (activate BLR based factorization):           0\n",
      "                          ICNTL(36) (choice of BLR factorization variant):        0\n",
      "                          ICNTL(38) (estimated compression rate of LU factors):   333\n",
      "                          CNTL(1) (relative pivoting threshold):      0.01 \n",
      "                          CNTL(2) (stopping criterion of refinement): 1.49012e-08 \n",
      "                          CNTL(3) (absolute pivoting threshold):      0. \n",
      "                          CNTL(4) (value of static pivoting):         -1. \n",
      "                          CNTL(5) (fixation for null pivots):         0. \n",
      "                          CNTL(7) (dropping parameter for BLR):       0. \n",
      "                          RINFO(1) (local estimated flops for the elimination after analysis): \n",
      "                            [0] 6.41935e+08 \n",
      "                          RINFO(2) (local estimated flops for the assembly after factorization): \n",
      "                            [0]  2.43105e+06 \n",
      "                          RINFO(3) (local estimated flops for the elimination after factorization): \n",
      "                            [0]  6.41935e+08 \n",
      "                          INFO(15) (estimated size of (in MB) MUMPS internal data for running numerical factorization): \n",
      "                          [0] 22\n",
      "                          INFO(16) (size of (in MB) MUMPS internal data used during numerical factorization): \n",
      "                            [0] 22\n",
      "                          INFO(23) (num of pivots eliminated on this processor after factorization): \n",
      "                            [0] 4913\n",
      "                          RINFOG(1) (global estimated flops for the elimination after analysis): 6.41935e+08 \n",
      "                          RINFOG(2) (global estimated flops for the assembly after factorization): 2.43105e+06 \n",
      "                          RINFOG(3) (global estimated flops for the elimination after factorization): 6.41935e+08 \n",
      "                          (RINFOG(12) RINFOG(13))*2^INFOG(34) (determinant): (0.,0.)*(2^0)\n",
      "                          INFOG(3) (estimated real workspace for factors on all processors after analysis): 1881395\n",
      "                          INFOG(4) (estimated integer workspace for factors on all processors after analysis): 56990\n",
      "                          INFOG(5) (estimated maximum front size in the complete tree): 663\n",
      "                          INFOG(6) (number of nodes in the complete tree): 378\n",
      "                          INFOG(7) (ordering option effectively used after analysis): 2\n",
      "                          INFOG(8) (structural symmetry in percent of the permuted matrix after analysis): 100\n",
      "                          INFOG(9) (total real/complex workspace to store the matrix factors after factorization): 1881395\n",
      "                          INFOG(10) (total integer space store the matrix factors after factorization): 56990\n",
      "                          INFOG(11) (order of largest frontal matrix after factorization): 663\n",
      "                          INFOG(12) (number of off-diagonal pivots): 0\n",
      "                          INFOG(13) (number of delayed pivots after factorization): 0\n",
      "                          INFOG(14) (number of memory compress after factorization): 0\n",
      "                          INFOG(15) (number of steps of iterative refinement after solution): 0\n",
      "                          INFOG(16) (estimated size (in MB) of all MUMPS internal data for factorization after analysis: value on the most memory consuming processor): 22\n",
      "                          INFOG(17) (estimated size of all MUMPS internal data for factorization after analysis: sum over all processors): 22\n",
      "                          INFOG(18) (size of all MUMPS internal data allocated during factorization: value on the most memory consuming processor): 22\n",
      "                          INFOG(19) (size of all MUMPS internal data allocated during factorization: sum over all processors): 22\n",
      "                          INFOG(20) (estimated number of entries in the factors): 1881395\n",
      "                          INFOG(21) (size in MB of memory effectively used during factorization - value on the most memory consuming processor): 19\n",
      "                          INFOG(22) (size in MB of memory effectively used during factorization - sum over all processors): 19\n",
      "                          INFOG(23) (after analysis: value of ICNTL(6) effectively used): 0\n",
      "                          INFOG(24) (after analysis: value of ICNTL(12) effectively used): 1\n",
      "                          INFOG(25) (after factorization: number of pivots modified by static pivoting): 0\n",
      "                          INFOG(28) (after factorization: number of null pivots encountered): 0\n",
      "                          INFOG(29) (after factorization: effective number of entries in the factors (sum over all processors)): 1881395\n",
      "                          INFOG(30, 31) (after solution: size in Mbytes of memory used during solution phase): 21, 21\n",
      "                          INFOG(32) (after analysis: type of analysis done): 1\n",
      "                          INFOG(33) (value used for ICNTL(8)): 7\n",
      "                          INFOG(34) (exponent of the determinant if determinant is requested): 0\n",
      "                          INFOG(35) (after factorization: number of entries taking into account BLR factor compression - sum over all processors): 1881395\n",
      "                          INFOG(36) (after analysis: estimated size of all MUMPS internal data for running BLR in-core - value on the most memory consuming processor): 0\n",
      "                          INFOG(37) (after analysis: estimated size of all MUMPS internal data for running BLR in-core - sum over all processors): 0\n",
      "                          INFOG(38) (after analysis: estimated size of all MUMPS internal data for running BLR out-of-core - value on the most memory consuming processor): 0\n",
      "                          INFOG(39) (after analysis: estimated size of all MUMPS internal data for running BLR out-of-core - sum over all processors): 0\n",
      "              linear system matrix = precond matrix:\n",
      "              Mat Object: 1 MPI processes\n",
      "                type: seqaij\n",
      "                rows=4913, cols=4913\n",
      "                total: nonzeros=126785, allocated nonzeros=126785\n",
      "                total number of mallocs used during MatSetValues calls=0\n",
      "                  not using I-node routines\n",
      "          linear system matrix = precond matrix:\n",
      "          Mat Object: (firedrake_4_mg_coarse_assembled_) 1 MPI processes\n",
      "            type: seqaij\n",
      "            rows=4913, cols=4913\n",
      "            total: nonzeros=126785, allocated nonzeros=126785\n",
      "            total number of mallocs used during MatSetValues calls=0\n",
      "              not using I-node routines\n",
      "        linear system matrix = precond matrix:\n",
      "        Mat Object: 1 MPI processes\n",
      "          type: python\n",
      "          rows=4913, cols=4913\n",
      "              Python: firedrake.matrix_free.operators.ImplicitMatrixContext\n",
      "            Firedrake matrix-free operator ImplicitMatrixContext\n",
      "    Down solver (pre-smoother) on level 1 -------------------------------\n",
      "      KSP Object: (firedrake_4_mg_levels_1_) 1 MPI processes\n",
      "        type: chebyshev\n",
      "          eigenvalue estimates used:  min = 0.19855, max = 2.18404\n",
      "          eigenvalues estimate via gmres min 0.0436521, max 1.9855\n",
      "          eigenvalues estimated using gmres with translations  [0. 0.1; 0. 1.1]          KSP Object: (firedrake_4_mg_levels_1_esteig_) 1 MPI processes\n",
      "            type: gmres\n",
      "              restart=30, using Classical (unmodified) Gram-Schmidt Orthogonalization with no iterative refinement\n",
      "              happy breakdown tolerance 1e-30\n",
      "            maximum iterations=10, initial guess is zero\n",
      "            tolerances:  relative=1e-12, absolute=1e-50, divergence=10000.\n",
      "            left preconditioning\n",
      "            using PRECONDITIONED norm type for convergence test\n",
      "          estimating eigenvalues using noisy right hand side\n",
      "        maximum iterations=10, nonzero initial guess\n",
      "        tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.\n",
      "        left preconditioning\n",
      "        using NONE norm type for convergence test\n",
      "      PC Object: (firedrake_4_mg_levels_1_) 1 MPI processes\n",
      "        type: jacobi\n",
      "          type DIAGONAL\n",
      "        linear system matrix = precond matrix:\n",
      "        Mat Object: 1 MPI processes\n",
      "          type: python\n",
      "          rows=35937, cols=35937\n",
      "              Python: firedrake.matrix_free.operators.ImplicitMatrixContext\n",
      "            Firedrake matrix-free operator ImplicitMatrixContext\n",
      "    Up solver (post-smoother) same as down solver (pre-smoother)\n",
      "    Down solver (pre-smoother) on level 2 -------------------------------\n",
      "      KSP Object: (firedrake_4_mg_levels_2_) 1 MPI processes\n",
      "        type: chebyshev\n",
      "          eigenvalue estimates used:  min = 0.199079, max = 2.18987\n",
      "          eigenvalues estimate via gmres min 0.0476982, max 1.99079\n",
      "          eigenvalues estimated using gmres with translations  [0. 0.1; 0. 1.1]          KSP Object: (firedrake_4_mg_levels_2_esteig_) 1 MPI processes\n",
      "            type: gmres\n",
      "              restart=30, using Classical (unmodified) Gram-Schmidt Orthogonalization with no iterative refinement\n",
      "              happy breakdown tolerance 1e-30\n",
      "            maximum iterations=10, initial guess is zero\n",
      "            tolerances:  relative=1e-12, absolute=1e-50, divergence=10000.\n",
      "            left preconditioning\n",
      "            using PRECONDITIONED norm type for convergence test\n",
      "          estimating eigenvalues using noisy right hand side\n",
      "        maximum iterations=10, nonzero initial guess\n",
      "        tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.\n",
      "        left preconditioning\n",
      "        using NONE norm type for convergence test\n",
      "      PC Object: (firedrake_4_mg_levels_2_) 1 MPI processes\n",
      "        type: jacobi\n",
      "          type DIAGONAL\n",
      "        linear system matrix = precond matrix:\n",
      "        Mat Object: (firedrake_4_) 1 MPI processes\n",
      "          type: python\n",
      "          rows=274625, cols=274625\n",
      "              Python: firedrake.matrix_free.operators.ImplicitMatrixContext\n",
      "            Firedrake matrix-free operator ImplicitMatrixContext\n",
      "    Up solver (post-smoother) same as down solver (pre-smoother)\n",
      "    linear system matrix = precond matrix:\n",
      "    Mat Object: (firedrake_4_) 1 MPI processes\n",
      "      type: python\n",
      "      rows=274625, cols=274625\n",
      "          Python: firedrake.matrix_free.operators.ImplicitMatrixContext\n",
      "        Firedrake matrix-free operator ImplicitMatrixContext\n",
      "Runtime : 21.422321319580078\n",
      "Error   : 1.9322475679750423e-05\n"
     ]
    }
   ],
   "source": [
    "u_h.assign(0)\n",
    "telescope_factor = 1 # Set to number of nodes!\n",
    "fmg_matfree_telescope = {\n",
    "    \"snes_view\": None,\n",
    "    \"mat_type\": \"matfree\",\n",
    "    \"ksp_type\": \"preonly\",\n",
    "    \"pc_type\": \"mg\",\n",
    "    \"pc_mg_log\": None,\n",
    "    \"pc_mg_type\": \"full\",\n",
    "    \"mg_levels_ksp_type\": \"chebyshev\",\n",
    "    \"mg_levels_ksp_max_it\": 10,\n",
    "    \"mg_levels_pc_type\": \"jacobi\",\n",
    "    \"mg_coarse_pc_type\": \"python\",\n",
    "    \"mg_coarse_pc_python_type\": \"firedrake.AssembledPC\",\n",
    "    \"mg_coarse_assembled\": {\n",
    "        \"mat_type\": \"aij\",\n",
    "        \"pc_type\": \"telescope\",\n",
    "        \"pc_telescope_reduction_factor\": telescope_factor,\n",
    "        \"pc_telescope_subcomm_type\": \"contiguous\",\n",
    "        \"telescope_pc_type\": \"lu\",\n",
    "        \"telescope_pc_factor_mat_solver_type\": \"mumps\"\n",
    "    }\n",
    "}\n",
    "run_solve(problem, fmg_matfree_telescope)\n",
    "parprint(\"Error   :\", errornorm(truth, u_h))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c92943",
   "metadata": {},
   "source": [
    "## Running on HPC\n",
    "\n",
    "To run these examples on HPC, the Firedrake code must be a Python script. You can download this notebook as a Python script in an interactive Jupyter notebook by clicking  `File > Download as > Python (.py)`.\n",
    "\n",
    "The code must run through a job scheduler using another script. An example job script suitable for running on ARCHER2 is provided below.\n",
    "\n",
    "To use this script change the account (`-A`) to your account, change the number of nodes (`--node=`) to the number of nodes you want to use and the time (`-t`) as appropriate, it is currently set to 10 _minutes_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edaf573d",
   "metadata": {},
   "source": [
    "```bash\n",
    "#!/bin/bash\n",
    "#SBATCH -p standard\n",
    "#SBATCH -A account\n",
    "#SBATCH -J firedrake\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --cpus-per-task=1\n",
    "#SBATCH -t 0:10:00\n",
    "\n",
    "export VENV_NAME=firedrake_08_2021\n",
    "export WORK=/work/e682/shared/firedrake_tarballs/firedrake_08_2021/\n",
    "export FIREDRAKE_TEMP=firedrake_tmp\n",
    "export LOCAL_BIN=$WORK\n",
    "\n",
    "myScript=\"HPC_demo.py\"\n",
    "\n",
    "module load epcc-job-env\n",
    "\n",
    "# Activate Firedrake venv (activate once on first node, extract once per node)\n",
    "source $LOCAL_BIN/firedrake_activate.sh\n",
    "srun --ntasks-per-node 1 $LOCAL_BIN/firedrake_activate.sh\n",
    "\n",
    "# Run Firedrake script\n",
    "srun --ntasks-per-node 128 $VIRTUAL_ENV/bin/python ${myScript}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf8e1fc",
   "metadata": {},
   "source": [
    "Finally, if you named your jobscript `jobscript.slm`, then it can be submitted to the queue by running the following command on ARCHER2:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d28917b",
   "metadata": {},
   "source": [
    "```bash\n",
    "sbatch jobscript.slm\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd2ea03",
   "metadata": {},
   "source": [
    "You can see your job's progress through the queue using:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aad5278",
   "metadata": {},
   "source": [
    "``` bash\n",
    "squeue -u $USER\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de92d5e",
   "metadata": {},
   "source": [
    "If you need to cancel a job for any reason, you can pass your job ID number as an argument to the scancel command:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd14f6b",
   "metadata": {},
   "source": [
    "``` bash\n",
    "scancel 123456\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c2827c",
   "metadata": {},
   "source": [
    "## Main exercise\n",
    "\n",
    "Perform a convergence study for the Poisson problem above, using degree 2 Lagrange elements. To do this solve the problem on a range of different mesh sizes. The cell diameter on the finest mesh in a multigrid hierarchy is given by $h = \\frac{\\sqrt{2}}{N}$, where $N = N_x \\times 2^{N_{ref}}$ is the number of cells along one edge of the cube on the finest grid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f284430",
   "metadata": {},
   "source": [
    "a)\n",
    "\n",
    "For this exercise we will repeatedly double $N$ (to half the value of $h$), and measure the error for each solution. Start by picking a suitable coarse mesh size (`Nx`) and number of mesh refinements (`Nref`) to achieve the desired fine mesh size `N`:\n",
    "\n",
    "| N =  | 8 | 16 | 32 | 64 | 128 | 256 | 512 |\n",
    "|------|---|----|----|----|-----|-----|-----|\n",
    "| Nx   |   |    | 8  |\n",
    "| Nref |   |    | 2  |\n",
    "\n",
    "Throughout the exercise we have already entered appropriate values into the table. These values correspond to the case presented in the notebook.\n",
    "\n",
    "The values for `Nx` and `Nref` are not unique, for instance to get a fine mesh with N = 256, we could choose `Nx = 256`, `Nref = 0` or `Nx = 8`, `Nref = 5`. However, if we use the former, we cannot use multigrid to solve the problem since there will be no mesh hierarchy! Think about how many multigrid levels are appropriate for the size of problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fb7931",
   "metadata": {},
   "source": [
    "b)\n",
    " \n",
    "Next approximate the number of DOFs for each problem using the guide in the **How big?** section above. Using the total number of DOFs work out how many processes would be appropriate for solving the problem (try to pick a power of 2) and hence how many nodes.\n",
    "\n",
    "| N =       | 8 | 16 | 32 | 64 | 128 | 256 | 512 |\n",
    "|-----------|---|----|----|----|-----|-----|-----|\n",
    "| DOFs      |   |    | 274625 |\n",
    "| Processes |   |    | 4  |\n",
    "| Nodes     |   |    | 1  |\n",
    "\n",
    "If you didn't complete the exercise in the **How big?** section, you can use $DOFs = (2N + 1)^3$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88877d5",
   "metadata": {},
   "source": [
    "c)\n",
    "\n",
    "For each problem size create a Firedrake problem and solve using the functions provided above. Record the error, alongside the cell diameter h in the table below. You will need to copy the code from this notebook, or download it as a Python file and modify as appropriate. \n",
    "\n",
    "Additionally, you will need to modify the jobscript to fit the size of problem you are solving. Both the Python script and jobscript need to be changed to suit the problem size!\n",
    "\n",
    "| N =   | 8 | 16 | 32 | 64 | 128 | 256 | 512 |\n",
    "|-------|---|----|----|----|-----|-----|-----|\n",
    "| h     |   |    | 0.044 |\n",
    "| Error |   |    | 2.33E-06 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43035fd",
   "metadata": {},
   "source": [
    "d)\n",
    "\n",
    "Plot the error against h and measure the rate of convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5d73a6",
   "metadata": {},
   "source": [
    "**Hints:**\n",
    "- You don't need much compute power to solve small problems on coarse meshes, these will likely fit on one node.\n",
    "- Remember to make you job big enough for the number of processes that you run:\n",
    "    - Each MPI rank must own at least one cell in the mesh\n",
    "    - Firedrake performs better when there are more than 50000 DOFs per rank"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Firedrake",
   "language": "python",
   "name": "firedrake"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
