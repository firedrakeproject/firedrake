{%- macro reduction_op(arg, lvalue, rvalue) -%}
{%- if(arg._is_INC) -%}
{{lvalue}} += {{rvalue}};
{%- elif(arg._is_MIN) -%}
if ( {{rvalue}} < {{lvalue}} ) {
    {{lvalue}} = {{rvalue}};
}
{%- elif(arg._is_MAX) -%}
if ( {{rvalue}} > {{lvalue}} ) {
    {{lvalue}} = {{rvalue}};
}
{%- endif -%}
{%- endmacro -%}

{%- macro reduction_kernel(arg) -%}
__device__ void {{ arg.data.name }}_reduction_kernel (
    volatile {{ arg.data.ctype }} *reduction_result,
    {{ arg.data.ctype }} input_value)
{
    extern __shared__ volatile {{ arg.data.ctype }} temp[];
    {{ arg.data.ctype }} dat_t;
    int tid = threadIdx.x;
    __syncthreads();
    temp[tid] = input_value;
    __syncthreads();

    // Fixup non-power of 2 blockDim
    // blockDim.x/2 rounded up to a power of 2
    int d = 1 << (31 - __clz((int)blockDim.x - 1));

    if ( tid + d < blockDim.x ) {
        dat_t = temp[tid + d];
        {{ reduction_op(arg, 'input_value', 'dat_t')|indent(8) }}
        temp[tid] = input_value;
    }

    // Reductions with more than one warp

    for ( d >>= 1; d > {{ launch.WARPSIZE }}; d >>= 1 ) {
        __syncthreads();
        if ( tid < d ) {
            dat_t = temp[tid + d];
            {{ reduction_op(arg, 'input_value', 'dat_t')|indent(12) }}
            temp[tid] = input_value;
        }
    }

    // intra-warp reduction
    __syncthreads();
    if ( tid < {{ launch.WARPSIZE }} ) {
        for ( ; d > 0; d >>= 1 ) {
            if ( tid < d ) {
                dat_t = temp[tid + d];
                {{ reduction_op(arg, 'input_value', 'dat_t')|indent(16) }}
                temp[tid] = input_value;
            }
        }
        // Update global reduction var
        if ( tid == 0 ) {
            {{ reduction_op(arg, '*reduction_result', 'input_value')|indent(12) }}
        }
    }
}
{%- endmacro -%}

{%- macro reduction_init(arg) -%}
{%- if (arg._is_INC) -%}
{{ arg.data.name }}_l[idx] = ({{arg.ctype}})0;
{%- else -%}
{{ arg.data.name }}_l[idx] = {{arg.data.name}}[idx + blockIdx.x * {{arg.data.cdim}}];
{%- endif -%}
{%- endmacro -%}

{%- macro kernel_stub() -%}
__global__ void {{ parloop._stub_name }} (
    int set_size,
    {% for arg in parloop._unique_args -%}
    {{ arg.ctype }} *{{arg.data.name}},
    {% endfor -%}
    int *ind_map,
    short *loc_map,
    int *ind_sizes,
    int *ind_offs,
    int block_offset,
    int *blkmap,
    int *offset,
    int *nelems,
    int *nthrcol,
    int *thrcol,
    int nblocks)
{
    extern __shared__ char shared[];

    {%- for arg in parloop._unique_indirect_dat_args %}
    __shared__ int *{{arg.data.name}}_map;
    __shared__ int {{arg.data.name}}_size;
    __shared__ {{arg.ctype}} * {{arg.data.name}}_s;
    {%- endfor %}
    __shared__ int nelems2, ncolor;
    __shared__ int nelem, offset_b;

    {%- for arg in parloop._inc_indirect_dat_args %}
    {{arg.ctype}} {{arg.data.name}}{{arg.idx}}_l[{{arg.data.cdim}}];
    {%- endfor %}

    {%- for arg in parloop._global_reduction_args %}
    {{arg.ctype}} {{arg.data.name}}_l[{{arg.data.cdim}}];
    {% endfor %}

    {% for arg in parloop._global_reduction_args %}
    for ( int idx = 0; idx < {{arg.data.cdim}}; ++idx ) {
        {{ reduction_init(arg) }}
    }
    {% endfor %}

    if (blockIdx.x + blockIdx.y * gridDim.x >= nblocks) return;
    if (threadIdx.x == 0) {
        int blockId = blkmap[blockIdx.x + blockIdx.y * gridDim.x + block_offset];
        nelem = nelems[blockId];
        offset_b = offset[blockId];

        nelems2 = blockDim.x * (1 + (nelem - 1)/blockDim.x);
        ncolor = nthrcol[blockId];

        {% for arg in parloop._unique_indirect_dat_args -%}
        {{arg.data.name}}_size = ind_sizes[{{loop.index0}} + blockId * {{loop.length}}];
        {{arg.data.name}}_map = &ind_map[{{arg._which_indirect}} * set_size] + ind_offs[{{loop.index0}} + blockId * {{loop.length}}];
        {% endfor %}
        int nbytes = 0;
        {% for arg in parloop._unique_indirect_dat_args -%}
        {{arg.data.name}}_s = ({{arg.ctype}} *) &shared[nbytes];
        {%- if (not loop.last) %}
        nbytes += ROUND_UP({{arg.data.name}}_size * sizeof({{arg.ctype}}) * {{arg.data.cdim}});
        {% endif -%}
        {% endfor %}
    }

    __syncthreads();

    // Copy into shared memory
    {% for arg in parloop._unique_read_indirect_dat_args %}
    for ( int idx = threadIdx.x; idx < {{arg.data.name}}_size * {{arg.data.cdim}}; idx += blockDim.x ) {
        {{arg.data.name}}_s[idx] = {{arg.data.name}}[idx % {{arg.data.cdim}} + {{arg.data.name}}_map[idx / {{arg.data.cdim}}] * {{arg.data.cdim}}];
    }
    {% endfor -%}

    {% for arg in parloop._unique_inc_indirect_dat_args %}
    for ( int idx = threadIdx.x; idx < {{arg.data.name}}_size * {{arg.data.cdim}}; idx += blockDim.x ) {
        {{arg.data.name}}_s[idx] = ({{arg.ctype}})0;
    }
    {% endfor %}

    __syncthreads();
    // process set elements

    for ( int idx = threadIdx.x; idx < nelems2; idx += blockDim.x ) {
        int col2 = -1;
        if ( idx < nelem ) {
            // initialise locals
            {% for arg in parloop._inc_indirect_dat_args %}
            for ( int idx2 = 0; idx2 < {{arg.data.cdim}}; ++idx2 ) {
                {{arg.data.name}}{{arg.idx}}_l[idx2] = ({{arg.ctype}})0;
            }
            {% endfor %}

            {{parloop.kernel.name}}(
                {%- set comma = joiner(",") -%}
                {%- for arg in parloop.args -%}
                {{ comma() }}
                {{ arg._indirect_kernel_arg_name('idx') }}
                {%- endfor -%}
                );


            col2 = thrcol[idx + offset_b];
        }

        for ( int col = 0; col < ncolor; ++col ) {
            if ( col2 == col ) {
                {%- for arg in parloop._inc_indirect_dat_args %}
                {% set tmp = 'loc_map[' ~ arg._which_indirect ~ ' * set_size + idx + offset_b]' -%}
                for ( int idx2 = 0; idx2 < {{arg.data.cdim}}; ++idx2) {
                    {{arg.data.name}}_s[idx2 + {{tmp}}*{{arg.data.cdim}}] += {{arg.data.name}}{{arg.idx}}_l[idx2];
                }
                {%- endfor %}
            }
            __syncthreads();
        }
    }
    // Write to global

    {%- for arg in parloop._unique_written_indirect_dat_args %}
    for ( int idx = threadIdx.x; idx < {{arg.data.name}}_size * {{arg.data.cdim}}; idx += blockDim.x ) {
        {% if arg._is_INC -%}
        {%- set op = '+=' -%}
        {%- else -%}
        {%- set op = '=' -%}
        {%- endif -%}
        {{arg.data.name}}[idx % {{arg.data.cdim}} + {{arg.data.name}}_map[idx/{{arg.data.cdim}}] * {{arg.data.cdim}}] {{op}} {{arg.data.name}}_s[idx];
    }
    {% endfor %}

    // Reductions
    {% for arg in parloop._global_reduction_args %}
    for ( int idx = 0; idx < {{ arg.data.cdim}}; ++idx ) {
        {{ arg.data.name }}_reduction_kernel(&{{arg.data.name}}[idx + blockIdx.x * {{arg.data.cdim}}], {{arg.data.name}}_l[idx]);
    }
    {% endfor %}
}

{%- endmacro -%}

{% for c in constants -%}
{{ c._format_declaration() }}
{% endfor %}
{%- if parloop._has_soa %}
#define OP2_STRIDE(array, idx) (array)[op2stride * (idx)]
{% endif %}
#define ROUND_UP(bytes) (((bytes) + 15) & ~15)

{{ parloop.kernel.code }}

{% for arg in parloop._global_reduction_args -%}
{{ reduction_kernel(arg) }}
{% endfor %}

{{ kernel_stub() }}
